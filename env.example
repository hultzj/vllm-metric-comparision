# =============================================================================
# vLLM vs TGI Benchmark Configuration
# =============================================================================
# Copy this file to .env and fill in your values:
#   cp env.example .env
# =============================================================================

# Hugging Face API Token (required for gated models like Llama)
# Get yours at: https://huggingface.co/settings/tokens
HF_TOKEN=your hugging face key

# Model Configuration
# Use Mistral (no approval required) or Llama (requires HuggingFace approval)
MODEL_ID=mistralai/Mistral-7B-Instruct-v0.3
# MODEL_ID=meta-llama/Llama-3.1-8B-Instruct  # Requires acceptance at huggingface.co

# Local cache directory for model weights
HF_CACHE_DIR=./models

# Grafana admin password
GRAFANA_PASSWORD=benchmark123

# API Key for external access (generate your own secure key)
# This enables OpenAI-compatible API authentication
VLLM_API_KEY=sk-your-secret-api-key-here

# GPU Cost Configuration (for ROI calculations)
# L4: ~$0.80/hour, L40S: ~$1.50/hour, A100: ~$3.00/hour
GPU_COST_PER_HOUR=0.80
GPU_NAME=L4
GPU_VRAM_GB=24

