# =============================================================================
# vLLM vs TGI Benchmark Configuration
# =============================================================================
# Copy this file to .env and fill in your values:
#   cp env.example .env
# =============================================================================

# Hugging Face API Token (required for gated models like Llama)
# Get yours at: https://huggingface.co/settings/tokens
HF_TOKEN=your hugging face key

# Model Configuration
# Use Mistral (no approval required) or Llama (requires HuggingFace approval)
MODEL_ID=mistralai/Mistral-7B-Instruct-v0.3
# MODEL_ID=meta-llama/Llama-3.1-8B-Instruct  # Requires acceptance at huggingface.co

# Local cache directory for model weights
HF_CACHE_DIR=./models

# Grafana admin password
GRAFANA_PASSWORD=benchmark123 # notsecret

# API Key for external access (generate your own secure key)
# This enables OpenAI-compatible API authentication
VLLM_API_KEY=sk-your-secret-api-key-here

# GPU Cost Configuration (for ROI calculations)
# AWS g6e.xlarge (L40S): ~$1.70/hour on-demand
# AWS g5.xlarge (A10G): ~$1.00/hour on-demand  
# AWS p4d.24xlarge (8x A100): ~$32.77/hour on-demand
GPU_COST_PER_HOUR=1.70
GPU_NAME=L40S
GPU_VRAM_GB=46

